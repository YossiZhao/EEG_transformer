{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "806273aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from typing import List, Union\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import torchonn as onn\n",
    "# from torchonn.models import ONNBaseModel\n",
    "# from torchonn.op.mzi_op import project_matrix_to_unitary\n",
    "# from torchonn.layers import MZILinear\n",
    "# from torchonn.models import ONNBaseModel\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.types import Device, _size\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import init\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fc6071",
   "metadata": {},
   "source": [
    "### Initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "737da267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Init logging\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)  # Use the current module's name\n",
    "logger.setLevel(logging.DEBUG)\n",
    "handler = logging.StreamHandler()\n",
    "# formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "# handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "test_num = 0.95  # Replace with your actual accuracy calculation\n",
    "logger.info(f\"Current accuracy: {test_num:.2f}\")  # Log as info\n",
    "# logger.debug(\"Current accuracy: %.2f\", accuracy)  # Log as info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3e7db",
   "metadata": {},
   "source": [
    "## Load encodered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce71bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_dir = './data/train_label.csv'\n",
    "train_data_dir = './data/encodered_csv/train/'\n",
    "\n",
    "eval_label_dir = './data/eval_label.csv'\n",
    "eval_data_dir = './data/encodered_csv/eval/'\n",
    "\n",
    "label_dic = {'normal':0, 'abnormal':1}\n",
    "\n",
    "\n",
    "class customDataset(Dataset):\n",
    "    def __init__(self, data_dir, label_dir, transform=None):\n",
    "#         self.annotations = pd.read_csv(label_dir)\n",
    "        self.data_dir = data_dir   # './data/origin_csv/train'\n",
    "        self.transform = transform\n",
    "        self.files = os.listdir(self.data_dir)\n",
    "        self.annotations = pd.read_csv(label_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_path = os.path.join(self.data_dir, self.files[index])\n",
    "        data = pd.read_csv(data_path)\n",
    "        data = torch.tensor(data.values, dtype=torch.float32)\n",
    "        file_name = self.files[index]\n",
    "        \n",
    "        label = torch.tensor(int(label_dic[self.annotations.iloc[index,1]]))\n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "            \n",
    "        return (data.t(), label, file_name)\n",
    "\n",
    "# dataset = customDataset(data_dir=data_dir, label_dir=label_dir)\n",
    "train_dataset = customDataset(data_dir=train_data_dir, label_dir=train_label_dir)\n",
    "eval_dataset = customDataset(data_dir=eval_data_dir, label_dir=eval_label_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00cf000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "batch_size = 128\n",
    "step = 0\n",
    "init_lr = 1e-4\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, \\\n",
    "                                  shuffle=True)\n",
    "\n",
    "eval_loader = DataLoader(dataset=eval_dataset, shuffle=True)\n",
    "\n",
    "def poly_lr_scheduler(optimizer, init_lr, iter, lr_decay_iter=1,\n",
    "                      max_iter=0, power=0.9):\n",
    "    \"\"\"Polynomial decay of learning rate\n",
    "        :param init_lr is base learning rate\n",
    "        :param iter is a current iteration\n",
    "        :param lr_decay_iter how frequently decay occurs, default is 1\n",
    "        :param max_iter is number of maximum iterations\n",
    "        :param power is a polymomial power\n",
    "    \"\"\"\n",
    "    if max_iter == 0:\n",
    "        raise Exception(\"MAX ITERATION CANNOT BE ZERO!\")\n",
    "    if iter % lr_decay_iter or iter > max_iter:\n",
    "        return optimizer\n",
    "    lr = init_lr * (1 - iter / max_iter) ** power\n",
    "    logger.debug(f'lr=: {lr}')\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e00a00",
   "metadata": {},
   "source": [
    "## Train transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d8cbf0",
   "metadata": {},
   "source": [
    "#### define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3f051ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=4)\n",
    "# transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6).to('cuda')\n",
    "# out = encoder_layer(src)\n",
    "class transformer_classifier(nn.Module):\n",
    "    def __init__(self, input_size, classes):\n",
    "        super(transformer_classifier, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=4)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=6)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(input_size, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.transformer_encoder(x)\n",
    "        z = self.flatten(z)\n",
    "        y = self.linear(z)\n",
    "        return y\n",
    "    \n",
    "classifier = transformer_classifier(256*19, 2).to('cuda')\n",
    "optimizer = torch.optim.Adam(classifier.parameters(),betas=(0.9,0.9),lr=init_lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae9162b",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5260fccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lr=: 0.0001\n",
      "Epoch: 0, Train Loss: 0.6948822736740112, Val Loss: 0.6961, Accuracy: 47.57%\n",
      "lr=: 9.954988729320692e-05\n",
      "lr=: 9.909954834128343e-05\n",
      "lr=: 9.864898188698403e-05\n",
      "lr=: 9.819818665965754e-05\n",
      "lr=: 9.774716137503496e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train_loader = DataLoader(dataset=dataset, batch_size=batch_size, \\\n",
    "#                                   shuffle=True)\n",
    "\n",
    "# src = torch.rand(18, 128).to('cuda')\n",
    "# label = torch.tensor([0]).to('cuda')\n",
    "# x_hat = ae.encoder(data)\n",
    "\n",
    "min_loss = 1\n",
    "for epoch in range(epochs):\n",
    "    # Training loop\n",
    "    poly_lr_scheduler(optimizer, init_lr=init_lr, iter=epoch, max_iter=epochs)\n",
    "    for batch_index, (data,target,_) in enumerate(train_loader, 0):\n",
    "#     for batch_index, data in enumerate(train_loader, 0):\n",
    "        data, target = data.to('cuda'), target.to('cuda')\n",
    "        y = classifier(data)\n",
    "#         logger.debug(f\"y size:{y.shape}, tatget size{target.shape}\")\n",
    "        train_loss = criterion(y, target)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "#         logger.info(f'Epoch: {epoch+1}, Train Loss: {train_loss}')\n",
    "#     logger.info(f\"Training Loss: {loss}\")\n",
    "    if epoch%5==0:\n",
    "    # Validation loop\n",
    "#         classifier.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        accuracy = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_index, (data,target,_) in enumerate(eval_loader, 0):\n",
    "                data, target = data.to('cuda'), target.to('cuda')\n",
    "                outputs = classifier(data)\n",
    "                loss = criterion(outputs, target)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += target.size(0)  # Total number of samples\n",
    "                correct += (predicted == target).sum().item()  # Count correct predictions\n",
    "\n",
    "        val_loss /= len(eval_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        logger.info(f'Epoch: {epoch}, Train Loss: {train_loss}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.2f}%')\n",
    "                    \n",
    "    torch.save(classifier.state_dict(), './weights/transformer_params_latest.pth')\n",
    "    if train_loss < min_loss:\n",
    "        torch.save(classifier.state_dict(), './weights/transformer_params_best.pth')\n",
    "        min_loss = train_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c8c0eb",
   "metadata": {},
   "source": [
    "## Save transformer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735c1a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(), './weights/transformer_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bf706b",
   "metadata": {},
   "source": [
    "### Normalize dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ecd04212",
   "metadata": {},
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_data.reshape((len(train_data), -1)))\n",
    "X_test = scaler.fit_transform(val_data.reshape((len(val_data), -1)))   \n",
    "logger.debug(f'X_train shape: {X_train.shape}, X_test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765076ec",
   "metadata": {},
   "source": [
    "### Positional encoding"
   ]
  },
  {
   "cell_type": "raw",
   "id": "710b9862",
   "metadata": {},
   "source": [
    "def positional_encoding(max_length, d_model, model_type='sinusoidal'):\n",
    "    \"\"\"\n",
    "    Generates positional encodings for a given maximum sequence length and model dimensionality.\n",
    "\n",
    "    Args:\n",
    "        max_length (int): The maximum length of the sequence.\n",
    "        d_model (int): The dimensionality of the model.\n",
    "        model_type (str): The type of positional encoding to use. Defaults to 'sinusoidal'.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The positional encoding matrix of shape (max_length, d_model).\n",
    "    \"\"\"\n",
    "\n",
    "    if model_type == 'sinusoidal':\n",
    "        pe = np.zeros((max_length, d_model))\n",
    "        position = np.arange(0, max_length, dtype=np.float32).reshape(-1, 1)\n",
    "        div_term = np.exp(np.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = np.sin(position * div_term)\n",
    "        pe[:, 1::2] = np.cos(position * div_term)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model_type: {}\".format(model_type))\n",
    "\n",
    "    return pe\n",
    "\n",
    "pe_train = positional_encoding(X_train.shape[0], X_train.shape[1])\n",
    "pe_test = positional_encoding(X_test.shape[0], X_test.shape[1])\n",
    "# Add positional encoding to the signal\n",
    "X_train =  X_train + pe_train # Add corresponding row of pe matrix\n",
    "X_test =  X_test + pe_test\n",
    "\n",
    "# def positional_encoding(max_length, d_model, model_type='sinusoidal'):\n",
    "#     for i, signal in enumerate(signal_dataset):\n",
    "#         if len(signal) <= max_length:\n",
    "#             # Pad shorter signals with zeros\n",
    "#             signal = np.pad(signal, (0, max_length - len(signal)), mode='constant')\n",
    "#         else:\n",
    "#             # Truncate longer signals\n",
    "#             signal = signal[:max_length]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e603c3",
   "metadata": {},
   "source": [
    "### Build Optimizer and lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6920793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(test_losses, label='test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25915e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = []\n",
    "predictions_test =  []\n",
    "with torch.no_grad():\n",
    "    predictions_train = model(X_train_1, X_train_2)\n",
    "    predictions_test = model(X_test_1, X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_multiclass(pred_arr,original_arr):\n",
    "    if len(pred_arr)!=len(original_arr):\n",
    "        return False\n",
    "    pred_arr = pred_arr.numpy()\n",
    "    original_arr = original_arr.numpy()\n",
    "    final_pred= []\n",
    "    # we will get something like this in the pred_arr [32.1680,12.9350,-58.4877]\n",
    "    # so will be taking the index of that argument which has the highest value here 32.1680 which corresponds to 0th index\n",
    "    for i in range(len(pred_arr)):\n",
    "        final_pred.append(np.argmax(pred_arr[i]))\n",
    "    final_pred = np.array(final_pred)\n",
    "    count = 0\n",
    "    #here we are doing a simple comparison between the predicted_arr and the original_arr to get the final accuracy\n",
    "    for i in range(len(original_arr)):\n",
    "        if final_pred[i] == original_arr[i]:\n",
    "            count+=1\n",
    "    return count/len(final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee0f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = get_accuracy_multiclass(predictions_train.cpu(),y_train.cpu())\n",
    "test_acc  = get_accuracy_multiclass(predictions_test.cpu(),y_test.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b6f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Training Accuracy: {round(train_acc*100,3)}\")\n",
    "logger.info(f\"Test Accuracy: {round(test_acc*100,3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "varInspector": {
   "cols": {
    "lenName": "50",
    "lenType": "50",
    "lenVar": "80"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "144px",
    "left": "1169px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
