{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "806273aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from typing import List, Union\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import torchonn as onn\n",
    "# from torchonn.models import ONNBaseModel\n",
    "# from torchonn.op.mzi_op import project_matrix_to_unitary\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.types import Device, _size\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch.nn import init\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch.utils.data import DataLoader\n",
    "# from torchonn.layers import MZILinear\n",
    "# from torchonn.models import ONNBaseModel\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fc6071",
   "metadata": {},
   "source": [
    "### Initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "737da267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Current accuracy: %0.95\n"
     ]
    }
   ],
   "source": [
    "# Init logging\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)  # Use the current module's name\n",
    "logger.setLevel(logging.DEBUG)\n",
    "handler = logging.StreamHandler()\n",
    "# formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "# handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)\n",
    "acc_example = 0.95  # Replace with your actual accuracy calculation\n",
    "logger.info(f\"Current accuracy: %{acc_example}\")  # Log as info\n",
    "# logger.debug(\"Current accuracy: %.2f\", accuracy)  # Log as info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d328770",
   "metadata": {},
   "source": [
    "## Auto-Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3e7db",
   "metadata": {},
   "source": [
    "#### Load raw data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7aea2e3e",
   "metadata": {},
   "source": [
    "data_dir = './data/origin_csv/train'\n",
    "files = os.listdir(data_dir)\n",
    "logger.debug(\"Num of files: %\", len(files))\n",
    "label_dir = './data/label.csv'\n",
    "annotations = pd.read_csv(label_dir)\n",
    "logger.debug(\"Num of files: %\", annotations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba68f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDataset(Dataset):\n",
    "    def __init__(self, data_dir, label_dir, transform=None):\n",
    "#         self.annotations = pd.read_csv(label_dir)\n",
    "        self.data_dir = data_dir   # './data/origin_csv/train'\n",
    "        self.transform = transform\n",
    "        self.files = os.listdir(self.data_dir)\n",
    "        self.annotations = pd.read_csv(label_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_path = os.path.join(self.data_dir, self.files[index])\n",
    "        data = pd.read_csv(data_path)\n",
    "        data = torch.tensor(data.values, dtype=torch.float32)\n",
    "        file_name = self.files[index]\n",
    "        \n",
    "        label = torch.tensor(int(label_dic[self.annotations.iloc[index,1]]))\n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "            \n",
    "        return (data.t(), label, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ce71bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_dir = './data/train_label.csv'\n",
    "train_data_dir = './data/origin_csv/train/'\n",
    "\n",
    "eval_label_dir = './data/eval_label.csv'\n",
    "eval_data_dir = './data/origin_csv/eval/'\n",
    "\n",
    "label_dic = {'normal':0, 'abnormal':1}\n",
    "\n",
    "    \n",
    "# transform = transforms.Compose([\n",
    "#     transforms.MinMaxScaler(feature_range=(0, 1)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "train_dataset = customDataset(data_dir=train_data_dir, label_dir=train_label_dir)\n",
    "eval_dataset = customDataset(data_dir=eval_data_dir, label_dir=eval_label_dir)\n",
    "combined_dataset = ConcatDataset([train_dataset, eval_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3347e263",
   "metadata": {},
   "source": [
    "#### Define auto-encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b74620f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# define\n",
    "\n",
    "class Mat_mul(nn.Module):\n",
    "    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
    "\n",
    "    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
    "\n",
    "    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
    "\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n",
    "          dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n",
    "        - Output: :math:`(*, H_{out})` where all but the last dimension\n",
    "          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
    "\n",
    "    Attributes:\n",
    "        weight: the learnable weights of the module of shape\n",
    "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
    "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
    "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
    "                If :attr:`bias` is ``True``, the values are initialized from\n",
    "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
    "                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = nn.Linear(20, 30)\n",
    "        >>> input = torch.randn(128, 20)\n",
    "        >>> output = m(input)\n",
    "        >>> print(output.size())\n",
    "        torch.Size([128, 30])\n",
    "    \"\"\"\n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "#         self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
    "        self.weight = Parameter(torch.empty((in_features, out_features), **factory_kwargs))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\n",
    "        # https://github.com/pytorch/pytorch/issues/57109\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return input @ self.weight + self.bias\n",
    "#         return torch.mul(input, self.weight, self.bias)\n",
    "    \n",
    "    \n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            Mat_mul(input_size, hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.encoder_2 = nn.Sequential(\n",
    "            Mat_mul(int(input_size/2), hidden_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            Mat_mul(hidden_size, input_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder_2 = nn.Sequential(\n",
    "            Mat_mul(int(input_size/2), input_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "#         z = self.encoder_2(z)\n",
    "        x_hat = self.decoder(z)\n",
    "#         x_hat = self.decoder_2(x_hat)\n",
    "        return x_hat\n",
    "\n",
    "\n",
    "# x = torch.randn(100, 10)\n",
    "\n",
    "# define ae model\n",
    "ae = AutoEncoder(input_size=1000, hidden_size=256).to('cuda')\n",
    "\n",
    "\n",
    "# use ae encoder\n",
    "# z = ae.encoder(x)\n",
    "\n",
    "# print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551aaf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_lr = 1e-4\n",
    "epochs = 100\n",
    "batch_size = 4096\n",
    "step = 0\n",
    "\n",
    "dataloader = DataLoader(dataset=combined_dataset, batch_size=batch_size, \\\n",
    "                                  shuffle=True)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(ae.parameters(),betas=(0.9,0.9),lr=init_lr)\n",
    "\n",
    "def poly_lr_scheduler(optimizer, init_lr, iter, lr_decay_iter=1,\n",
    "                      max_iter=0, power=0.9):\n",
    "    \"\"\"Polynomial decay of learning rate\n",
    "        :param init_lr is base learning rate\n",
    "        :param iter is a current iteration\n",
    "        :param lr_decay_iter how frequently decay occurs, default is 1\n",
    "        :param max_iter is number of maximum iterations\n",
    "        :param power is a polymomial power\n",
    "    \"\"\"\n",
    "    if max_iter == 0:\n",
    "        raise Exception(\"MAX ITERATION CANNOT BE ZERO!\")\n",
    "    if iter % lr_decay_iter or iter > max_iter:\n",
    "        return optimizer\n",
    "    lr = init_lr * (1 - iter / max_iter) ** power\n",
    "    logger.debug(f'lr=: {lr}')\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692f289e",
   "metadata": {},
   "source": [
    "## Train auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1ac77cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lr=: 0.0001\n",
      "epoch=0, loss=0.11845755577087402\n",
      "lr=: 9.909954834128343e-05\n",
      "epoch=1, loss=0.11250562220811844\n",
      "lr=: 9.819818665965754e-05\n",
      "epoch=2, loss=0.11015499383211136\n",
      "lr=: 9.729590473501306e-05\n",
      "epoch=3, loss=0.10887461155653\n",
      "lr=: 9.63926921258551e-05\n",
      "epoch=4, loss=0.10820388048887253\n",
      "lr=: 9.548853816214998e-05\n",
      "epoch=5, loss=0.10739194601774216\n",
      "lr=: 9.458343193786322e-05\n",
      "epoch=6, loss=0.10710445791482925\n",
      "lr=: 9.367736230317176e-05\n",
      "epoch=7, loss=0.1076795905828476\n",
      "lr=: 9.277031785633283e-05\n",
      "epoch=8, loss=0.10783097892999649\n",
      "lr=: 9.186228693518995e-05\n",
      "epoch=9, loss=0.10684892535209656\n",
      "lr=: 9.095325760829622e-05\n",
      "epoch=10, loss=0.10539495199918747\n",
      "lr=: 9.004321766563289e-05\n",
      "epoch=11, loss=0.10772541165351868\n",
      "lr=: 8.91321546089e-05\n",
      "epoch=12, loss=0.10658790916204453\n",
      "lr=: 8.822005564135439e-05\n",
      "epoch=13, loss=0.10598741471767426\n",
      "lr=: 8.73069076571686e-05\n",
      "epoch=14, loss=0.10725656151771545\n",
      "lr=: 8.639269723028191e-05\n",
      "epoch=15, loss=0.10709816217422485\n",
      "lr=: 8.547741060271343e-05\n",
      "epoch=16, loss=0.10613204538822174\n",
      "lr=: 8.45610336723042e-05\n",
      "epoch=17, loss=0.10617362707853317\n",
      "lr=: 8.36435519798534e-05\n",
      "epoch=18, loss=0.10622978210449219\n",
      "lr=: 8.272495069561094e-05\n",
      "epoch=19, loss=0.10573436319828033\n",
      "lr=: 8.180521460508584e-05\n",
      "epoch=20, loss=0.10661905258893967\n",
      "lr=: 8.088432809412662e-05\n",
      "epoch=21, loss=0.10663658380508423\n",
      "lr=: 7.996227513322693e-05\n",
      "epoch=22, loss=0.10558361560106277\n",
      "lr=: 7.903903926100555e-05\n",
      "epoch=23, loss=0.10626405477523804\n",
      "lr=: 7.811460356680608e-05\n",
      "epoch=24, loss=0.10657929629087448\n",
      "lr=: 7.718895067235705e-05\n",
      "epoch=25, loss=0.10566092282533646\n",
      "lr=: 7.626206271242883e-05\n",
      "epoch=26, loss=0.10492752492427826\n",
      "lr=: 7.533392131441787e-05\n",
      "epoch=27, loss=0.10540951788425446\n",
      "lr=: 7.440450757678327e-05\n",
      "epoch=28, loss=0.10681357979774475\n",
      "lr=: 7.347380204625457e-05\n",
      "epoch=29, loss=0.10590998828411102\n",
      "lr=: 7.2541784693722e-05\n",
      "epoch=30, loss=0.10584770143032074\n",
      "lr=: 7.160843488871356e-05\n",
      "epoch=31, loss=0.10614163428544998\n",
      "lr=: 7.067373137235415e-05\n",
      "epoch=32, loss=0.10627955198287964\n",
      "lr=: 6.973765222869263e-05\n",
      "epoch=33, loss=0.10597238689661026\n",
      "lr=: 6.880017485427283e-05\n",
      "epoch=34, loss=0.10580054670572281\n",
      "lr=: 6.78612759258125e-05\n",
      "epoch=35, loss=0.10668323189020157\n",
      "lr=: 6.692093136584148e-05\n",
      "epoch=36, loss=0.10632360726594925\n",
      "lr=: 6.597911630613657e-05\n",
      "epoch=37, loss=0.10586240142583847\n",
      "lr=: 6.50358050487742e-05\n",
      "epoch=38, loss=0.10670625418424606\n",
      "lr=: 6.409097102460522e-05\n",
      "epoch=39, loss=0.1058056503534317\n",
      "lr=: 6.314458674893553e-05\n",
      "epoch=40, loss=0.10625600069761276\n",
      "lr=: 6.219662377417515e-05\n",
      "epoch=41, loss=0.10539302974939346\n",
      "lr=: 6.124705263919325e-05\n",
      "epoch=42, loss=0.10594725608825684\n",
      "lr=: 6.029584281508921e-05\n",
      "epoch=43, loss=0.10657817870378494\n",
      "lr=: 5.93429626470586e-05\n",
      "epoch=44, loss=0.10594502091407776\n",
      "lr=: 5.838837929199803e-05\n",
      "epoch=45, loss=0.10615817457437515\n",
      "lr=: 5.743205865145341e-05\n",
      "epoch=46, loss=0.1053132712841034\n",
      "lr=: 5.6473965299470985e-05\n",
      "epoch=47, loss=0.10564161837100983\n",
      "lr=: 5.551406240486037e-05\n",
      "epoch=48, loss=0.1044841781258583\n",
      "lr=: 5.455231164732059e-05\n",
      "epoch=49, loss=0.10651817917823792\n",
      "lr=: 5.358867312681466e-05\n",
      "epoch=50, loss=0.10569852590560913\n",
      "lr=: 5.262310526550319e-05\n",
      "epoch=51, loss=0.10599792748689651\n",
      "lr=: 5.16555647014613e-05\n",
      "epoch=52, loss=0.10552684217691422\n",
      "lr=: 5.0686006173304735e-05\n",
      "epoch=53, loss=0.10497524589300156\n",
      "lr=: 4.9714382394737164e-05\n",
      "epoch=54, loss=0.10606669634580612\n",
      "lr=: 4.874064391789954e-05\n",
      "epoch=55, loss=0.10634929686784744\n",
      "lr=: 4.776473898425048e-05\n",
      "epoch=56, loss=0.10595633089542389\n",
      "lr=: 4.678661336153e-05\n",
      "epoch=57, loss=0.10501130670309067\n",
      "lr=: 4.580621016515332e-05\n",
      "epoch=58, loss=0.10531158745288849\n",
      "lr=: 4.4823469662140946e-05\n",
      "epoch=59, loss=0.10544904321432114\n",
      "lr=: 4.38383290554087e-05\n",
      "epoch=60, loss=0.10692692548036575\n",
      "lr=: 4.2850722245909175e-05\n",
      "epoch=61, loss=0.10527845472097397\n",
      "lr=: 4.186057956972281e-05\n",
      "epoch=62, loss=0.10585067421197891\n",
      "lr=: 4.086782750672989e-05\n",
      "epoch=63, loss=0.10625313222408295\n",
      "lr=: 3.987238835693844e-05\n",
      "epoch=64, loss=0.106926828622818\n",
      "lr=: 3.887417987987635e-05\n",
      "epoch=65, loss=0.10551251471042633\n",
      "lr=: 3.787311489165393e-05\n",
      "epoch=66, loss=0.10526541620492935\n",
      "lr=: 3.6869100813333195e-05\n",
      "epoch=67, loss=0.10487193614244461\n",
      "lr=: 3.586203916306077e-05\n",
      "epoch=68, loss=0.10580477863550186\n",
      "lr=: 3.485182498298004e-05\n",
      "epoch=69, loss=0.10555607080459595\n",
      "lr=: 3.3838346190164985e-05\n",
      "epoch=70, loss=0.10515249520540237\n",
      "lr=: 3.282148283862538e-05\n",
      "epoch=71, loss=0.10561217367649078\n",
      "lr=: 3.180110627669995e-05\n",
      "epoch=72, loss=0.10548445582389832\n",
      "lr=: 3.077707818072785e-05\n",
      "epoch=73, loss=0.10642948746681213\n",
      "lr=: 2.9749249441556528e-05\n",
      "epoch=74, loss=0.10581286996603012\n",
      "lr=: 2.8717458874925874e-05\n",
      "epoch=75, loss=0.10532081872224808\n",
      "lr=: 2.768153171967635e-05\n",
      "epoch=76, loss=0.10559042543172836\n",
      "lr=: 2.664127787853039e-05\n",
      "epoch=77, loss=0.10575971752405167\n",
      "lr=: 2.5596489844146203e-05\n",
      "epoch=78, loss=0.10564640164375305\n",
      "lr=: 2.4546940237185756e-05\n",
      "epoch=79, loss=0.10512366890907288\n",
      "lr=: 2.3492378861760376e-05\n",
      "epoch=80, loss=0.10609321296215057\n",
      "lr=: 2.2432529154608912e-05\n",
      "epoch=81, loss=0.10534888505935669\n",
      "lr=: 2.136708386445385e-05\n",
      "epoch=82, loss=0.10585757344961166\n",
      "lr=: 2.0295699742231397e-05\n",
      "epoch=83, loss=0.10548745095729828\n",
      "lr=: 1.9217990943702904e-05\n",
      "epoch=84, loss=0.10592854768037796\n",
      "lr=: 1.8133520731367457e-05\n",
      "epoch=85, loss=0.1056041419506073\n",
      "lr=: 1.7041790893331676e-05\n",
      "epoch=86, loss=0.10535804182291031\n",
      "lr=: 1.5942228040916463e-05\n",
      "epoch=87, loss=0.10409342497587204\n",
      "lr=: 1.4834165549752874e-05\n",
      "epoch=88, loss=0.10597041249275208\n",
      "lr=: 1.3716819274517819e-05\n",
      "epoch=89, loss=0.10634741932153702\n",
      "lr=: 1.2589254117941671e-05\n",
      "epoch=90, loss=0.10541374236345291\n",
      "lr=: 1.1450336728854525e-05\n",
      "epoch=91, loss=0.10636959969997406\n",
      "lr=: 1.0298666348361787e-05\n",
      "epoch=92, loss=0.10612193495035172\n",
      "lr=: 9.132469616782773e-06\n",
      "epoch=93, loss=0.10529351979494095\n",
      "lr=: 7.949432487547622e-06\n",
      "epoch=94, loss=0.10662925243377686\n",
      "lr=: 6.746414238367822e-06\n",
      "epoch=95, loss=0.10652967542409897\n",
      "lr=: 5.518918645844864e-06\n",
      "epoch=96, loss=0.10593728721141815\n",
      "lr=: 4.259995391188707e-06\n",
      "epoch=97, loss=0.10575143992900848\n",
      "lr=: 2.9575152732566297e-06\n",
      "epoch=98, loss=0.10505596548318863\n",
      "lr=: 1.5848931924611145e-06\n",
      "epoch=99, loss=0.10576929897069931\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    'model_state_dict': ae.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'learning_rate': optimizer.param_groups[0]['lr']\n",
    "}\n",
    "min_loss = 0.5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    poly_lr_scheduler(optimizer, init_lr=init_lr, iter=epoch, max_iter=epochs)\n",
    "    for batch_index, (data,_,_) in enumerate(dataloader, 0):\n",
    "        data = data.to('cuda')\n",
    "#         data = data.to('cuda')\n",
    "        x_hat = ae(data)\n",
    "#         logger.debug(\"x_hat, shape=%\", x_hat.shape)\n",
    "        loss = criterion(x_hat, data)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    logger.info(f'epoch={epoch}, loss={loss}')\n",
    "    torch.save(model_params, './weights/model_params_latest.pth')\n",
    "    if min_loss > loss:\n",
    "        torch.save(model_params, './weights/model_params_best.pth')\n",
    "        min_loss = loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e00a00",
   "metadata": {},
   "source": [
    "## Save auto-encoder"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e2e8b0ea",
   "metadata": {},
   "source": [
    "torch.save(ae.state_dict(), './weights/ae_model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d88992",
   "metadata": {},
   "source": [
    "## Load auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe13f07",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4137ee6",
   "metadata": {},
   "source": [
    "## auto-encoder inference for transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e490cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ae_infer(data_path:str, result_path:str, label_dir:str):\n",
    "    \n",
    "    if os.path.exists(result_path):\n",
    "        shutil.rmtree(result_path)\n",
    "    os.mkdir(result_path)\n",
    "    enc_dataset = customDataset(data_dir=str(data_path), label_dir=label_dir)\n",
    "    # Define column names (optional, but recommended)\n",
    "    channels = ['Fp1', 'Fp2', 'F3','F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2', 'F7', 'F8', 'T3', 'T4',\n",
    "                'T5', 'T6', 'Fz', 'Cz', 'Pz']\n",
    "    ae_inference  = AutoEncoder(input_size=1000, hidden_size=256).to('cuda')\n",
    "    ae_inference.load_state_dict(torch.load('./weights/ae_model_weights.pth'))\n",
    "    ae_inference.eval()\n",
    "    \n",
    "    for batch_index, (data,label,file_name) in enumerate(enc_dataset, 0):\n",
    "        data = data.to('cuda')\n",
    "        z = ae_inference.encoder(data)   # 19*500\n",
    "    #     z = ae_inference.encoder_2(z)   # 19*256\n",
    "    #     logger.debug(file_name)  # Log as info\n",
    "    #     logger.debug(z.shape)  # Log as info\n",
    "        z = z.t().cpu()\n",
    "        z = z.detach().numpy()\n",
    "        \n",
    "        df = pd.DataFrame(z, columns=channels)\n",
    "\n",
    "        # Save as CSV file\n",
    "        df.to_csv(result_path+file_name, index=False) \n",
    "\n",
    "#  training data encoding\n",
    "train_label_dir = './data/train_label.csv'\n",
    "train_data_dir = './data/origin_csv/train/'\n",
    "train_result_dir = './data/encodered_csv/train/'\n",
    "ae_infer(train_data_dir, train_result_dir, train_label_dir)\n",
    "\n",
    "#  evaluation data encoding\n",
    "eval_label_dir = './data/eval_label.csv'\n",
    "eval_data_dir = './data/origin_csv/eval/'\n",
    "eval_result_dir = './data/encodered_csv/eval/'\n",
    "ae_infer(eval_data_dir, eval_result_dir, eval_label_dir)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe3f6c37",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Load data first\n",
    "label_dir = './data/label.csv'\n",
    "data_dir = './data/origin_csv/train'\n",
    "result_dir = './data/encodered_csv/train/'\n",
    "label_dic = {'normal':0, 'abnormal':1}\n",
    "\n",
    "if os.path.exists(result_dir):\n",
    "    shutil.rmtree(result_dir)\n",
    "os.mkdir(result_dir)\n",
    "\n",
    "enc_dataset = customDataset(data_dir=data_dir, label_dir=label_dir)\n",
    "\n",
    "for batch_index, (data,label,file_name) in enumerate(enc_dataset, 0):\n",
    "    data = data.to('cuda')\n",
    "    z = ae_inference.encoder(data)   # 19*500\n",
    "#     z = ae_inference.encoder_2(z)   # 19*256\n",
    "#     logger.debug(file_name)  # Log as info\n",
    "#     logger.debug(z.shape)  # Log as info\n",
    "    z = z.t().cpu()\n",
    "    z = z.detach().numpy()\n",
    "\n",
    "# Define column names (optional, but recommended)\n",
    "    channels = ['Fp1', 'Fp2', 'F3','F4', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2', 'F7', 'F8', 'T3', 'T4',\n",
    "                'T5', 'T6', 'Fz', 'Cz', 'Pz']\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(z, columns=channels)\n",
    "\n",
    "    # Save as CSV file\n",
    "    df.to_csv(result_dir+file_name, index=False) \n",
    "\n",
    "    \n",
    "    # convert tensor to encodered .csv\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae9162b",
   "metadata": {},
   "source": [
    "#### Train transformer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1ecbd2f2",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# signal dataset in pytorch\n",
    "label_dir = './data/label.csv'\n",
    "data_dir = './data/csv/'\n",
    "\n",
    "\n",
    "class customDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "#         self.annotations = pd.read_csv(label_dir)\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.data_dir))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_path = os.path.join(self.data_dir, self.annotations.iloc[index,1])\n",
    "        data = pd.read_csv(data_path)\n",
    "#         y_label = torch.tensor(int(self.annotations.iloc[index,2]))\n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "            \n",
    "        return data\n",
    "\n",
    "dataset = customDataset(data_dir=data_dir)\n",
    "\n",
    "# train_set, test_set = torch.utils.data.random_split(dataset, [1067, 22000])\n",
    "\n",
    "# # For dataset\n",
    "# train_user = ['00007', '00008', '00009', '00010', '00014', '00015', '00016', \\\n",
    "#               '00017', '00018', '00019', '00020', '00021', \\\n",
    "#               '00022', '00027', '00028', '00030', '00031', '00032', '00033', '00036', \\\n",
    "#               '00037', '00038', '00039', '00040']\n",
    "# val_user = ['00024', '00025', '00026', '00034', '00035']\n",
    "# classes = {'dislike':0, 'like':1}\n",
    "\n",
    "# data_path = Path('../Neuromarketing/raw_preference')  # need to modify\n",
    "# # label_path = Path('./labels')\n",
    "# result_dir_path = '../Neuromarketing/result/'\n",
    "\n",
    "# if os.path.exists(result_dir_path):\n",
    "#     shutil.rmtree(result_dir_path)\n",
    "# os.mkdir(result_dir_path)\n",
    "\n",
    "# # Load data and labels\n",
    "# train_data_files = []\n",
    "# val_data_files = []\n",
    "# for file in os.listdir(data_path):\n",
    "#     for user in train_user:\n",
    "#         if file.startswith(user):\n",
    "#             train_data_files.append(file)\n",
    "#     for user in val_user:\n",
    "#         if file.startswith(user):\n",
    "#             val_data_files.append(file)\n",
    "\n",
    "\n",
    "# # Preprocessing of training dataset\n",
    "\n",
    "# # train_data_files = os.listdir(data_path)\n",
    "# bands = {'raw': 701, 'zero':0, 'delta':1, 'theta':2, 'alpha':3, 'beta':4, 'gamma':5}\n",
    "# # bands_len = [0, 38, 76, 146, 279, 538]\n",
    "# band = 'raw'\n",
    "\n",
    "# if band == 'raw':\n",
    "#     train_data = np.zeros((len(train_data_files), bands['raw'], 4), dtype='float16')\n",
    "# else:\n",
    "#     train_data = np.zeros((len(train_data_files),14,bands_len[bands[band]]-(bands_len[bands[band]-1])), dtype='float16')\n",
    "\n",
    "\n",
    "# # We have column features saved as csv files but sklearn needs row features.\n",
    "# train_labels = np.zeros(len(train_data_files))\n",
    "# # print(data.shape)\n",
    "# for i,file in enumerate(train_data_files):\n",
    "# #     print(i)\n",
    "#     if band == 'raw':\n",
    "#         df = pd.read_csv(data_path / file)\n",
    "# #         print(df.loc[:, [\"O1\", \"O2\",\"F3\", \"F4\"]])\n",
    "#         df = pd.DataFrame(df.loc[:, [\"O1\",\"O2\",\"F3\",\"F4\"]]).to_numpy()\n",
    "#         train_data[i,:] = df\n",
    "# #         print(train_data)\n",
    "#     else:\n",
    "#     # load data with specific band\n",
    "#         train_data[i,:] = pd.read_csv(data_path / file, header=None)[bands_len[bands[band]-1]:bands_len[bands[band]]].T\n",
    "    \n",
    "# #     train_data[i,:] = preprocessing.normalize(train_data[i,:], norm='l2')\n",
    "#     pre_label = file.split('_')[1]\n",
    "#     train_labels[i] = classes[pre_label]\n",
    "# #     data = pd.read_csv(data_path / file, header=None)\n",
    "\n",
    "# logger.debug(f'train_data shape: {train_data.shape}, train_label shape: {train_labels.shape}')\n",
    "\n",
    "\n",
    "# # Preprocessing for test dataset\n",
    "# if band == 'raw':\n",
    "#     val_data = np.zeros((len(val_data_files), bands['raw'], 4), dtype='float16')\n",
    "# else:\n",
    "#     val_data = np.zeros((len(train_data_files),14,bands_len[bands[band]]-(bands_len[bands[band]-1])), dtype='float16')\n",
    "# val_labels = np.zeros(len(val_data_files))\n",
    "# for i,file in enumerate(val_data_files):\n",
    "# #     print(i)\n",
    "#     if band == 'raw':\n",
    "#     # load data with fixed length\n",
    "#         df = pd.read_csv(data_path / file)\n",
    "#         df = pd.DataFrame(df.loc[:, [\"O1\", \"O2\", \"F3\", \"F4\"]]).to_numpy()\n",
    "#         val_data[i,:] = df\n",
    "#     else:\n",
    "#     # load data with specific band\n",
    "#         val_data[i,:] = pd.read_csv(data_path / file, header=None)[bands_len[bands[band]-1]:bands_len[bands[band]]].T\n",
    "\n",
    "    \n",
    "# #     val_data[i,:] = preprocessing.normalize(val_data[i,:], norm='l2')\n",
    "#     pre_label = file.split('_')[1]\n",
    "#     val_labels[i] = classes[pre_label]\n",
    "# #     data = pd.read_csv(data_path / file, header=None)\n",
    "# logger.debug(f'val_data shape: {val_data.shape}, val_label shape: {val_labels.shape}')\n",
    "\n",
    "# import pickle\n",
    "# import joblib\n",
    "\n",
    "# from sklearn import svm\n",
    "\n",
    "# svm_classifier = svm.SVC(kernel='rbf', C=1.0, tol=1e-6)\n",
    "# print(train_data.shape)\n",
    "# train_data = train_data.reshape((1,-1))\n",
    "# print(train_data.shape)\n",
    "# svm_classifier.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bf706b",
   "metadata": {},
   "source": [
    "### Normalize dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ecd04212",
   "metadata": {},
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_data.reshape((len(train_data), -1)))\n",
    "X_test = scaler.fit_transform(val_data.reshape((len(val_data), -1)))   \n",
    "logger.debug(f'X_train shape: {X_train.shape}, X_test shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765076ec",
   "metadata": {},
   "source": [
    "### Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49473e60",
   "metadata": {},
   "source": [
    "### raw data to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787eb2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "X_train_1 = X_train[:, 0:X_train.shape[1]//2]\n",
    "X_train_2 = X_train[:, X_train.shape[1]//2:]\n",
    "\n",
    "\n",
    "X_test_1 = X_test[:, 0:X_test.shape[1]//2]\n",
    "X_test_2 = X_test[:, X_test.shape[1]//2:]\n",
    "\n",
    "y_train = train_labels\n",
    "y_test = val_labels\n",
    "\n",
    "X_train_1 = torch.FloatTensor(X_train_1).to('cuda')\n",
    "X_train_2 = torch.FloatTensor(X_train_2).to('cuda')\n",
    "X_test_1 = torch.FloatTensor(X_test_1).to('cuda')\n",
    "X_test_2 = torch.FloatTensor(X_test_2).to('cuda')\n",
    "\n",
    "y_train = torch.LongTensor(y_train).to('cuda')\n",
    "y_test = torch.LongTensor(y_test).to('cuda')\n",
    "\n",
    "input_dim = X_train_1.shape[1]\n",
    "output_dim = 2\n",
    "logger.debug(f'X_train_1.shape: {X_train_1.shape}, X_train_2.shape: {X_train_2.shape}, y_train.shape: {y_train.shape},X_test_1.shape: {X_test_1.shape}, X_test_2.shape: {X_test_2.shape}, y_test.shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e603c3",
   "metadata": {},
   "source": [
    "### Build Optimizer and lr"
   ]
  },
  {
   "cell_type": "raw",
   "id": "009556f2",
   "metadata": {},
   "source": [
    "init_lr = 1e-5\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),betas=(0.9,0.9),lr=init_lr)\n",
    "\n",
    "def poly_lr_scheduler(optimizer, init_lr, iter, lr_decay_iter=1,\n",
    "                      \n",
    "                      max_iter=0, power=0.9):\n",
    "    \"\"\"Polynomial decay of learning rate\n",
    "        :param init_lr is base learning rate\n",
    "        :param iter is a current iteration\n",
    "        :param lr_decay_iter how frequently decay occurs, default is 1\n",
    "        :param max_iter is number of maximum iterations\n",
    "        :param power is a polymomial power\n",
    "    \"\"\"\n",
    "    if max_iter == 0:\n",
    "        raise Exception(\"MAX ITERATION CANNOT BE ZERO!\")\n",
    "    if iter % lr_decay_iter or iter > max_iter:\n",
    "        return optimizer\n",
    "    lr = init_lr * (1 - iter / max_iter) ** power\n",
    "    logger.debug(f'lr=: {lr}')\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42db0121",
   "metadata": {},
   "source": [
    "### define  training"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6f7daf3e",
   "metadata": {},
   "source": [
    "def train_network(model,optimizer,criterion,\n",
    "                  X_train_1,X_train_2,y_train,\n",
    "                  X_test_1,X_test_2,y_test,\n",
    "                  num_epochs,train_losses,test_losses):\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # update lr_rate\n",
    "#         poly_lr_scheduler(optimizer, init_lr=init_lr, iter=epoch, max_iter=num_epochs)\n",
    "        \n",
    "        #clear out the gradients from the last step loss.backward()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward feed\n",
    "        output_train = model(X_train_1, X_train_2)\n",
    "\n",
    "        #calculate the loss\n",
    "        loss_train = criterion(output_train, y_train)\n",
    "        \n",
    "\n",
    "\n",
    "        #backward propagation: calculate gradients\n",
    "        loss_train.backward()\n",
    "\n",
    "        #update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        output_test = model(X_test_1, X_test_2)\n",
    "        loss_test = criterion(output_test,y_test)\n",
    "\n",
    "        train_losses[epoch] = loss_train.item()\n",
    "        test_losses[epoch] = loss_test.item()\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            logger.info(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss_train.item():.4f}, Test Loss: {loss_test.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f39076",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95be763f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "train_losses = np.zeros(num_epochs)\n",
    "test_losses  = np.zeros(num_epochs)\n",
    "\n",
    "train_network(model,optimizer,criterion,\n",
    "              X_train_1,X_train_2,y_train,\n",
    "              X_test_1,X_test_2,y_test,\n",
    "              num_epochs,train_losses,test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6920793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(train_losses, label='train loss')\n",
    "plt.plot(test_losses, label='test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25915e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = []\n",
    "predictions_test =  []\n",
    "with torch.no_grad():\n",
    "    predictions_train = model(X_train_1, X_train_2)\n",
    "    predictions_test = model(X_test_1, X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d73c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_multiclass(pred_arr,original_arr):\n",
    "    if len(pred_arr)!=len(original_arr):\n",
    "        return False\n",
    "    pred_arr = pred_arr.numpy()\n",
    "    original_arr = original_arr.numpy()\n",
    "    final_pred= []\n",
    "    # we will get something like this in the pred_arr [32.1680,12.9350,-58.4877]\n",
    "    # so will be taking the index of that argument which has the highest value here 32.1680 which corresponds to 0th index\n",
    "    for i in range(len(pred_arr)):\n",
    "        final_pred.append(np.argmax(pred_arr[i]))\n",
    "    final_pred = np.array(final_pred)\n",
    "    count = 0\n",
    "    #here we are doing a simple comparison between the predicted_arr and the original_arr to get the final accuracy\n",
    "    for i in range(len(original_arr)):\n",
    "        if final_pred[i] == original_arr[i]:\n",
    "            count+=1\n",
    "    return count/len(final_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee0f9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = get_accuracy_multiclass(predictions_train.cpu(),y_train.cpu())\n",
    "test_acc  = get_accuracy_multiclass(predictions_test.cpu(),y_test.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b6f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Training Accuracy: {round(train_acc*100,3)}\")\n",
    "logger.info(f\"Test Accuracy: {round(test_acc*100,3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c23fab",
   "metadata": {},
   "source": [
    "### Evolutionary algorithm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eca19549",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define the task (binary classification)\n",
    "def generate_data(num_samples=100):\n",
    "    # Generate random data for a binary classification task\n",
    "    X = torch.randn(num_samples, 2)\n",
    "    y = (X[:, 0] * X[:, 1] > 0).float()  # Simple XOR-like classification\n",
    "    return X, y\n",
    "\n",
    "# Define a simple feedforward neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(10, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Define the evolutionary algorithm\n",
    "def evolutionary_algorithm(population_size, generations, mutation_rate):\n",
    "    # Initialize the population with random neural networks\n",
    "    population = [SimpleNN() for _ in range(population_size)]\n",
    "\n",
    "    for generation in range(generations):\n",
    "        # Evaluate the fitness of each individual in the population\n",
    "        fitness_scores = []\n",
    "        for individual in population:\n",
    "            X, y = generate_data()\n",
    "            predictions = individual(X)\n",
    "            loss = nn.BCELoss()(predictions, y.view(-1, 1))\n",
    "            fitness_scores.append(-loss.item())  # Negative because we want to maximize fitness\n",
    "\n",
    "        # Select the top-performing individuals as parents\n",
    "        selected_parents = np.argsort(fitness_scores)[-int(0.2 * population_size):]\n",
    "\n",
    "        # Create offspring through crossover and mutation\n",
    "        offspring = []\n",
    "        for _ in range(population_size - len(selected_parents)):\n",
    "            parent1 = population[np.random.choice(selected_parents)]\n",
    "            parent2 = population[np.random.choice(selected_parents)]\n",
    "            child = SimpleNN()\n",
    "\n",
    "            # Crossover (swap parameters)\n",
    "            for child_param, parent1_param, parent2_param in zip(child.parameters(), parent1.parameters(), parent2.parameters()):\n",
    "                crossover_mask = (torch.rand_like(child_param.data) > 0.5).float()\n",
    "                child_param.data = crossover_mask * parent1_param.data + (1 - crossover_mask) * parent2_param.data\n",
    "\n",
    "            # Mutation (randomly perturb parameters)\n",
    "            for child_param in child.parameters():\n",
    "                mutation_mask = (torch.rand_like(child_param.data) < mutation_rate).float()\n",
    "                mutation = torch.randn_like(child_param.data) * mutation_mask\n",
    "                child_param.data += mutation\n",
    "\n",
    "            offspring.append(child)\n",
    "\n",
    "        # Replace the old population with the new population\n",
    "        population = selected_parents + offspring\n",
    "\n",
    "    # Return the best-performing individual\n",
    "    best_individual = population[np.argmax(fitness_scores)]\n",
    "    return best_individual\n",
    "\n",
    "# Example usage\n",
    "best_model = evolutionary_algorithm(population_size=50, generations=50, mutation_rate=0.1)\n",
    "\n",
    "# Now you can use the best_model for further processing or inference\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "varInspector": {
   "cols": {
    "lenName": "50",
    "lenType": "50",
    "lenVar": "80"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122,
   "position": {
    "height": "144px",
    "left": "1169px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
